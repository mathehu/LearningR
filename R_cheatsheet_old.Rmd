---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
##Preparing workspace
```{r}
library(tidyverse)
library(lubridate)
library(magrittr)
library(dplyr)
library(tidyr)
```

##Checking directories
```{r}
dir()
getwd()
setwd() ##setwd("~/R/DataScienceIntro/datasciencecoursera")
setwd("../") ##moving up one level of the directory
setwd("./data")

file.exists("directoryName") ##check to see if directory exists -logical
dir.create("directoryName") ##will create directory if it doesn't exist
if(!file.exists("data")) {     ##same as above, checks if directory exists, otherwise creates one
  dir.create("data")
}

```


##Getting data
```{r}
# download file from internet
fileUrl <- "https://data.source.com/...."

download.file(fileUrl, destfile = "./data/filename.csv", method = "curl") ##"curl" only needed when downloading https data using Mac, on Windows default should work 

list.files("./data")

dateDownloaded <- date()    ##record for purpose of documentation

## Reading local files
read.table()  ###main function for reading data into R, reads data into RAM - big files might need to be split
read.table("./data/filename.csv", sep = ",", header = TRUE)

read.csv() ##as read.table but automatically assigns sep = ",", header = TRUE
read.csv('file', stringsAsFactors = F, header = T)
read.csv2()

read.xlsx()
nameData <- read.xlsx("./data/filename.xlsx", sheetIndex=1, header=TRUE)
read.xlsx2()   ##faster than read.xlsx but more unstable when reading subsets of rows

read_excel("") ##load library(readx1) first

##XLConnect package has more options for writing and manipulating Excel files
##Check out XLConnect vignette to start

##important parameters
quote ##tell R whether there are any quoted values, quote="", means no quotes
na.strings ## set the character that represents a missing value
skip ##number of lines to skip before starting to read
nrows ##how many rows to read of the file (e.g. nrows=10 reads 10 lines)

colIndex <- 2:3  ##reading only columns 2 to 3
rowIndex <- 1:4
read.xlsx("./data/fileName.xlsx", sheetIndex=1, colIndex=colIndex, rowIndex=rowIndex)

library(jsonlite)
jsonData <- fromJSON("F:\\Projekte\\DWH\\Auswertungen\\Psychotherapie\\Clearingstelle\\PriorizR_Datenanalyse\\Logs\\01.01.2021.json")
names(jsonData)   ##Looking at names of variables in the df
names(jsonData$GruppenfÃ¤higkeit)
head(jsonData)

fromJSON()

```


##Loading data
```{r}
data() ##load data(frame), i.e. load(mtcars)

```
##Transpose a dataframe
```{r}
library(data.table)
df_t <- transpose(df)
rownames(df_t) <- colnames(df)
colnames(df_t) <- rownames(df)

tibble()    ##from dataframe to table structure
```

##Looking at data
```{r}
cran ##just typing name of dataset shows entire dataset
str() ##overview of the structure of the data
str(mooc1$unformatted_date) ##uses str on only one column
class()
dim()
nrow()
ncol()
object.size() ##space of dataset in memory
names() ## column (i.e. variable) names
head() ##to preview the top of the dataset, default six rows; more when specified, eg. head(plants, 20)
tail() ####to preview the end of the dataset
View(noNA)    ### shows df as table in upper panel
unique()    ### list of all different unique values in the vector, dataframe, etc.
abs(x)    ## Betrag einer Zahl berechnen
slice_sample(mooc1, n=100)  ##takes a random sample of the data sate with n rows.
mooc1 %>%  arrange(desc(revenue)) %>% head(n=25) ##arrange the revenue colunn by descending value and show the top 25 rows

##%in%##
%in%  ##logical checks whether a value is in a vector, z.B. %in% v1 Result: T or F
!3 %in% v1 ##checks whether 3 is NOT in v1  
df5 <- j17i %>%
  filter(LineItem %in% c('Glass Mug', 'Gift Cards')) ##filters observations containing Glass Mug and Gift Cards
CustsThatPurchaseHighCostItems <- j17i %>%      ##filter names of customers who purchased high cost items
  filter(CarholderNames %in% highCostItems$CardholderName)

summary(df) ##overview of how each variable is distributed with descriptive statistics
summary(df$Price)
summary(df[, c('Price', 'Cost')])
summary(factor(df))   ##factor coerces string into numeric and returns number of observations for Price and Cost.
table(plants$Active_Growth_Period) ##how many times each value actually occurs in the data
```
# Manipulating data frames
```{r}
library(tidyverse)

# changing the class of a variable
mooc1_states$postal_code2 <- as.integer(mooc1_states$postal_code) ##changing the variable postal_code to an integer

# selecting only certain elements of a string variable
str_sub(1,5)   ##selecting only the first 5 elements of each cell in the column
mooc1_states$postal_code2 <- mooc1_states$postal_code %>% str_sub(1,5) ##doing the above and assigning the selected sting elements to a new df

library(tidyr)

##Pivoting df##
###Pivot longer -> collapse various column names into one column, needs to duplicate, i.e. date, many times
dailyLong <- daily %>%
  pivot_longer(cols = c(avgCost, maxPrice, transactionQuantity)) ##indicate which columns to use

dailyLong <- daily %>% ##same as above but gives new columns specific names
  pivot_longer(cols = c(avgCost:transactionQuantity)
               , names_to ='metrics'
               , values_to = 'vals')

###Pivot_wider
dailyWide <- dailyLong %>%
  pivot_wider(names_from = metrics, values_from = vals)

##Stacking and sorting data##
library(dplyr)
library(magrittr)

?bind_rows ##stacking different dfs together
allItems <- bind_rows(df1, df2, df3)

?bind_cols
allItems2 <- bind_cols(df1, df2, df3)

###Creating new columns
df1$addcolumn <- str_length(df1$addcolumn)  ##creats a new column "df1$addcolumn" and fills it with length of strings (str_length)

##Chaining functions together
###using the pipe operator: %>% -> this allows you to use add/use different functions in the same step
df1 <- j17i %>%
  filter(Cost > 11) %>%
  select(Cost, Price)

###using the two-way pipe operator
## using a two-way pipe operator, performs the function and puts the result back into the df
allItems <- allItems %<>%  
  arrange(desc(CardholderName), Time) ##ordering CardholderName by descending, Time by ascending

##Renaming columns
library(dplyr)
j17i_2 <- j17i_2 %>%    ##renaming the long column labels into cmu and cmt
  rename(cmu =contMarginPerUnit, cmt =contMarginTotal)

library(tidyverse)
mooc1_states <- mooc1_states %>% 
  rename(zip = postal_code2) ##renames postal_code2 to zip

##Relocating columns in a df
j17i_2 <- j7i_2 %>%
  relocate(cmu, cmt, .after = Quantity)  ##moving teh cmu and cmt columns to the right of the quantity column

# Place Numeric columns  before character columns
numFirst <- j17i_2 %>%
  relocate(where(is.numeric), .before = where(is.character))

#Joining dfs together
    ## first step is to identify a column on which to match the data - the primary key
library(dplyr)
library(magrittr)
library(lubridate)

n_distinct() ##checking whether all elements of a vector are unique, count the number of unique values in a set of vectors

# Reading in dfs
j17i <- read.csv('jan17Items.csv') %>%
  mutate(
    Time = ymd_hms(Time)
  )

j17w <- read.csv('jan17Weather.csv', sep = '\t') %>%
  mutate(
    date = ymd_hms(date)
  )
# Joining the dfs
## Overview of joining options:
   ##inner_join(): includes all rows in the first dataframe and the second dataframe.
   ##left_join(): includes all rows in the first dataframe and those in the 2nd df THAT MATCH UP!
   ## right_join(): includes all rows in the second dataframe.
   ##full_join(): includes all rows in the first dataframe or the second dataframe

  # left_join only keeps information in the left df, if there are different numbers of observations in the dfs
dailyLeft <- daily %>%
  left_join(j17w, by = 'date')  # join by primary key

mooc1_etl <- left_join(mooc1, mooc1_states, by='zip') #alternative to line above: joinin mooc1 with mooc1_states by the matched column zip

# right_join only keeps information in the right df
dailyRight <- daily %>%   
  right_join(j17w, by = 'date')

# inner join only keeps observations that are matched in both dfs
dailyInner <- daily %>%   
  inner_join(j17w, by = 'date')

# full join keeps all observations in both dfs
dailyFull <- daily %>%   
  full_join(j17w, by = 'date')

# Removing outliers
outliers <- mooc1 %>% filter(product_id == 17628) ## Creating a new df with only one product from the product_id column. see "==" to not assign 17625 to "product_id" but to tell T ro filter for 17625.
mooc1_clean <- mooc1 %>% filter(product_id != 17628) ##Creating a new df with all products in product_id except "!=" product 17628, thus removing outliers
summary(mooc1_clean$revenue) ##check the distribtion of the variable
```
## Order data using the forcats pacakge
```{r}
library(forcats)
fct_inorder() ##orders values by first appearance
fct_infreq() ##orders values by frequency
fct_inseq()   ####orders values by numeric value
fct_relevel() ###give specific order to values
fct_lump()  ##choose number of levels to display, lump others together in one level
```



#Visually explore data
```{r}
hist(df$Price)
boxplot(df$Price)
plot(df$Price)     ##Scatterplot
plot(dfw$TMIN, dfw$TMax)  ##Setting the names for the x and y axes 
plot(dfw$TMAX, type = 'l')   ## makeing a lince chart, setting type: options under ?plot
plot(dfw[,c('TMIN', 'TMax', 'Rain')])   ###plotting the values of these three colunns for comparison
plot(cars, xlim = c(10, 15)) ##Plot cars while limiting the x-axis to 10 through 15.

?par ##Set or Query Graphical Parameters
?points ##change the shape of the symbols in the plot
 


```
 
## Summe der Spalten
```{r}
Spaltensumme <- function(MFBer, removeNA = TRUE) {
  nc <- ncol(MFBer)                  ##figure out number of columns
   <- numeric(nc)    ##create vector(Spalte) that stores the means for the number of columns
  for(i in 1:nc) {
    sums[i] <- sum(MFBer[, i], na.rm=removeNA)
  }
    sums
}

```
## Missing values NAs
```{r}
df2 <- df%>%
  filter(!is.na(Price)) ##filter all NAs from a column
summary()  ##also returns number of NAs per column

is.na(c(3, 5, NA, 10)) ##finding out which value for NA is used
  bad <- is.na(x)
  x[!bad]     ##selects all elements of x that aren't NAs
filter(cran, !is.na(r_version)) ## gets all rows from a column without NAs

## Impute missing values
?ifelse
df3 <- df %>%
  mutate(
    Tax = ifelse(is.na(Tax), mean(Tax, na.rm =T), Tax)
  , TotalDue = NetTotal + Tax
)

## install dplyr and magrittr packages
library(dplyr)
library(magrittr)

## NAs across multiple vectors
  ##Vector y has NAs, Vector x has NAs

  good <- complete.cases(x, y)
  x[good]  ##contains no NAs
  y[good]

## NAs across dataframes
airquality[1:6, ]     ## I only want rows 1 to 6 from airquality dataframe, but without NAs
good <- complete.cases(airquality)
airquality[good, ][1:6, ]

```
## Measures of central tendency
```{r}
colMeans() ##Means of columns

 ##Durchschnitt der Spalten

columnmean <- function(y, removeNA = TRUE) {
  nc < ncol(y)                  ##figure out number of columns
  means <- numeric(nc)    ##create vector(Spalte) that stores the means for the number of columns
  for(i in 1:nc) {
    means[i]<- mean(y[, i], na.rm=removeNA)
  }
    means
}

mean(x, na.rm = TRUE)
``` 
## accessing parts of a document
````{r}
 variable[[1]]  ##prints the first part of variable
 
variable [[1]] [[1]] ##prints the first element of the first part of the variable
```
## Programmatically extract parts of an xml file
````{r}
xmlSApply(variableName, xmlValue)

  ##Extracting elements of a file using xPath
  ##Information on xPath on http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf
````
# Referencing and subsetting vectors by location ----
````{r}
V1[1]  ##gives me first value of df
V1[1:5]
V2 <- v1[c(1,5)]  ##creating new vector with elements of v1

## Referencing and subsetting dataframes by location
dfw[1:5, 1:3]
dfw[1:5, ]
dfw[.1]   #selecting only the first column
dfw1 <- dfw[1:5, c(1, 3)] ### selecting rows 1 to 5 but only columns 1 and 3 and creating new df with subset of dfw
ldf <- df[df$Price > 15, ]   ##creates new df, which contains only rows where the Price > 15
brdf <- df[df$Category %in% c("Beef", "Rice"), ]   ####creates new df, which contains only rows where category is either Beef or Rice
```
# Referencing and subsetting vectors by name
````{r}
library(dplyr)
dfwÂ§date
dfw[1:5, c('date', 'time')]

df$BarCode <- NULL  ## removing a specific column from the df

##removing data frame##
rm("mydf") 

tbl_df()  #creates a 'data frame tbl' that creates more informative and compact outputs when printed to console


##from here on commands from the dplyr Package!!!!!!!##

## select -> reduces widths of df and only returns the columns you want##
select() ##Select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right)
select(cran, ip_id, package, country) ##selects the columns ip_id, package, country
select(cran, r_arch:country) ##selects all columns from r_arch to country)
select(cran, -time) ##select columns without "time"
select(cran, -(X:size))  ####select without columns from "x" to "size"

mooc1_states <- mooc1_states %>% 
  select(-postal_code, -country_name)
mooc1_states <- mooc1_states %>% select(zip, state_province) ## use `select()` to reorder the columns and drop `state_province_code`. First, put variables in the order you want. Then, omit the ones you donât want. Thus, we drop `state_province_code` just by not mentioning it in the `select()` function.
```

package == "swirl" ## returns a vector of TRUEs and FALSEs. filter() then returns only the rows of cran corresponding to the TRUEs.

##filter -> filters out/reduces as many rows/observations as you want, the save the filtered version as a new df##
filter()  #selecting subset of rows#
filter(cran, r_version == "3.1.1", country == "US") ##You can specify as many conditions as you want, separated by commas.
higCostAndPice <- filter(j17i, Cost > 11 & Price > 13) ##filters all rows where Cost >11 and Price > 13
filter(cran, r_version <= "3.0.2", country == "IN") ##works with all logical comparisons, eg. <=
filter(cran, country == "US" | country == "IN") ##works with all logical comparisons, eg. | or
filter(cran, package == "swirl")   ##  to select all rows for which the package variable is equal to "swirl"

##to order the rows of a data set according to the values of a particular variable
arrange()  
  select(cran, r_arch:country)   ##select() all columns from size     through ip_id and store the result in cran2
  arrange(cran2, ip_id) ##orders the ROWS of cran2 so that ip_id is in ascending order
arrange(cran2, desc(ip_id))## arrange in descending order
arrange(cran2, package, ip_id) ##arrange by two variables
arrange(cran2, country, desc(r_version), ip_id)

allItems <- allItems %>%
  arrange(CardholderName, Time)  ##sort df first by CardholderName and then by Time

allItems <- allItems %<>%  ##using a two-way pipe
  arrange(desc(CardholderName), Time) ##ordering CardholderName by descending, Time by ascending
  

##Creating calculated fields##
mutate()  ##allItemsthe value of one or more | variables already in a data set
mutate(cran3, size_mb = size / 2^20)    ##to add a column called size_mb that contains the download size in  megabytes
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10) ##multiple columns (also building on each other) can be created in the same line
j171_2 <- j17i %>%    ##create two new columns contMarginPerUnit and contMarginTotal based on two different
  mutate(             ## calculations using the values in the Price, Cost and Quantity columns
    contMarginPerUnit = Price - Cost#
    , contMarginTotal = contMarginPerUnit * Quantity
  )
# returning only distinct items
df7 <- j17i %>%
  filter(cmt >40) %>%  ##filter only observations > 40
  select(LineItem) %>%  ##select only this column
  distinct() ##return each distinct value only once
df7

summarize()   ##collapses the dataset to a single row
summarize(cran, avg_bytes = mean(size)) ##yield the mean value of the size variable. label of the result 'avg_bytes' was chosen by author
##summarize() can give you the requested value FOR EACH group in your dataset
```

##Loops
```{r}
x <- c("a", "b", "c", "d")
for(i in 1:4) {
  print(x[i])
}

for(i in seq_along(x)) {             ##loops through the length of the vector
  print(x[i])
}

for(i in seq_len(nrow(x))) {                  ##nested loops, here to print all elements of a matrix
  for(j in seq_len(ncol(x))) {
    print(x[i, j])
  }
}
```
## Exporting data
```{r}
##Exporting data
write.xlsx  ##writes out an Excel file
toJson()
 
write_rds(mooc1_etl, 'mooc1_etl_done.rds', compress = 'gz')  ##rds files preserves format of columns (CSV does not) and safes space, particularly when using the compress function

```
## Manipulating character data
```{r}
library(stringr)
str_length(v1) ##length of all strings in v1
str_to_title(v1)   ##capitalises the first letter in every word
str_to_lower()   #makes all words start with lower letters, i.e. when the same words are sometimes written with small letters and sometimes with capitals
str_to_upper  ##capitalises all letters
str_detect(df, 'His')  #checks whether the word 'his' is in the df; case sensitive!
j171$kabob <- str_detect(j17i$lineItemLower, 'kabob') ##creates column "kabob' with T or F whether kabob is present
str_replace()
str_replace_all()
str_split(df, 'T', simplify = T) ###splits vector or df (column) at the indicated place, i.e. 'T', and create two columns (via the simplify argument)

```
#Manipulating data in date format##
```{r}
library(lubridate)

mooc1$date <- ymd(mooc1$unformatted_date) ##changing a column  of a dataset into a date format, other formats are dmy, etc.

mooc1$year <- year(mooc1$date) ##create a year column in the mooc1 dataset

mooc1$month <- month(mooc1$date, label=TRUE) ##create a month column in the mooc1 dataset; The label function returns names rather than numbers for months

daily <-  j17i %>%
  mutate(
    Time = ymd_hms(Time)  ##change Time chr string into dateTime object
    , date = round_date(Time, 'day')  #changes the time object an rounds it into 'days'
  ) %>%
  group_by(date) %>% #groups the df by date and aggregates (summarises) new columns into one df
  summarise(avgCost = mean(Cost, na.rm = T) # creates new columns avg cost, max price and number of ...
            , maxPrice = max(Price, na.rm = T) #... distinct transactions
            , transactionQuantity = n_distinct(TransactionNumber)) %>%
  ungroup()
```

